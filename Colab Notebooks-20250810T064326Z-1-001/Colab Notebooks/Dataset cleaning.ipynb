{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"mount_file_id":"1hQp1PlYdC6-g-8eCjD-A67WNUjpu0Sop","authorship_tag":"ABX9TyM36r09LPg2LnU2GULaMmrg"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nu4nJoSRYQ3t","executionInfo":{"status":"ok","timestamp":1754668644761,"user_tz":0,"elapsed":35327,"user":{"displayName":"Bruce Jnr","userId":"02572787615479852412"}},"outputId":"26761e3a-8626-4e42-804a-fe63c7372811"},"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n","Saved files:\n","- FEVER 5k: /content/drive/MyDrive/Colab Notebooks/merged_data/fever_5000.json\n","- TruthfulQA 5k paraphrased: /content/drive/MyDrive/Colab Notebooks/merged_data/truthfulqa_5000_paraphrased.json\n","- MERGED 10k: /content/drive/MyDrive/Colab Notebooks/merged_data/merged_10k.json\n","\n","Label counts:\n","  FEVER 5k:\n"," labels\n","1    3605\n","0    1395\n","Name: count, dtype: int64\n","  TQA 5k paraphrased:\n"," labels\n","1    5000\n","Name: count, dtype: int64\n","  Merged 10k:\n"," labels\n","1    8605\n","0    1395\n","Name: count, dtype: int64\n","\n","Also wrote gzipped copy: /content/drive/MyDrive/Colab Notebooks/merged_data/merged_10k.json.gz\n"]}],"source":["# =========================\n","# Colab: FEVER + TruthfulQA -> 10k merged JSON\n","# =========================\n","!pip install -q pandas numpy openpyxl\n","\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","import json, random\n","from pathlib import Path\n","import pandas as pd\n","import numpy as np\n","\n","# ---------- CONFIG: change these to where your files live ----------\n","INPUT_FEVER = Path(\"/content/drive/MyDrive/Colab Notebooks/train.jsonl\")      # FEVER jsonl\n","INPUT_TQA   = Path(\"/content/drive/MyDrive/Colab Notebooks/TruthfulQA.xlsx\")  # TruthfulQA xlsx or csv\n","\n","# Output directory (in Drive)\n","OUT_DIR = Path(\"/content/drive/MyDrive/Colab Notebooks/merged_data\")\n","OUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","OUT_FEVER_5K = OUT_DIR / \"fever_5000.json\"\n","OUT_TQA_5K   = OUT_DIR / \"truthfulqa_5000_paraphrased.json\"\n","OUT_MERGED   = OUT_DIR / \"merged_10k.json\"\n","\n","random.seed(42)\n","\n","def clean_text(x):\n","    if pd.isna(x):\n","        return \"\"\n","    s = str(x).strip()\n","    return \" \".join(s.split())\n","\n","# ---------- FEVER loader (jsonl) ----------\n","def load_fever_jsonl(path: Path) -> pd.DataFrame:\n","    rows = []\n","    with path.open(\"r\", encoding=\"utf-8\") as f:\n","        for line in f:\n","            line = line.strip()\n","            if not line:\n","                continue\n","            try:\n","                obj = json.loads(line)\n","            except Exception:\n","                continue\n","            question = obj.get(\"question\") or obj.get(\"claim\") or obj.get(\"statement\") or \"\"\n","            answer   = obj.get(\"answer\") or question  # for FEVER, answer ~ claim/statement\n","            label    = obj.get(\"label\")\n","            evidence = obj.get(\"evidence_text\") or obj.get(\"evidence\") or obj.get(\"page\") or \"\"\n","            rows.append({\n","                \"question\": clean_text(question),\n","                \"answer\": clean_text(answer),\n","                \"evidence\": clean_text(str(evidence).replace(\"_\", \" \")),\n","                \"raw_label\": label\n","            })\n","    df = pd.DataFrame(rows)\n","\n","    def map_label(x):\n","        if x is None: return np.nan\n","        s = str(x).strip().upper()\n","        if s == \"SUPPORTS\": return 1\n","        if s == \"REFUTES\":  return 0\n","        if s in {\"NEI\", \"NOT ENOUGH INFO\", \"NOT_ENOUGH_INFO\"}: return np.nan\n","        if s in {\"0\", \"1\"}: return int(s)\n","        return np.nan\n","\n","    df[\"labels\"] = df[\"raw_label\"].map(map_label)\n","    df = df.dropna(subset=[\"labels\"]).copy()   # drop NEI/unknown\n","    df[\"labels\"] = df[\"labels\"].astype(int)\n","    df[\"question\"] = df[\"question\"].fillna(\"\")\n","    df[\"answer\"]   = df[\"answer\"].fillna(df[\"question\"])\n","    df[\"evidence\"] = df[\"evidence\"].fillna(\"\")\n","    return df[[\"question\",\"answer\",\"evidence\",\"labels\"]]\n","\n","# ---------- TruthfulQA loader (xlsx or csv) ----------\n","def load_truthfulqa(path: Path) -> pd.DataFrame:\n","    if path.suffix.lower() in {\".xlsx\", \".xls\"}:\n","        df = pd.read_excel(path, sheet_name=0)\n","    else:\n","        df = pd.read_csv(path)\n","\n","    cols = {c.lower(): c for c in df.columns}\n","    # required\n","    q_col = cols.get(\"question\") or list(df.columns)[0]\n","    # try common answer col names\n","    a_col = (cols.get(\"answer\") or cols.get(\"model_answer\") or cols.get(\"response\")\n","             or cols.get(\"generated_answer\") or cols.get(\"best_answer\"))\n","    if a_col is None:\n","        # fallback to second column\n","        a_col = list(df.columns)[1]\n","\n","    # optional label col\n","    l_col = cols.get(\"label\") or cols.get(\"is_true\") or cols.get(\"target\") or cols.get(\"truth\")\n","\n","    df[\"question\"] = df[q_col].map(clean_text)\n","    df[\"answer\"]   = df[a_col].map(clean_text)\n","    df[\"evidence\"] = \"\"\n","\n","    if l_col and l_col in df.columns:\n","        raw = df[l_col].astype(str).str.strip().str.lower()\n","        mapping = {\n","            \"1\": 1, \"true\": 1, \"t\": 1, \"yes\": 1, \"y\": 1, \"factual\": 1, \"correct\": 1,\n","            \"0\": 0, \"false\": 0, \"f\": 0, \"no\": 0,  \"n\": 0, \"hallucinated\": 0, \"incorrect\": 0\n","        }\n","        df[\"labels\"] = raw.map(mapping)\n","        df = df.dropna(subset=[\"labels\"]).copy()\n","        df[\"labels\"] = df[\"labels\"].astype(int)\n","    else:\n","        # If no labels, assume factual=1 (you can change this)\n","        df[\"labels\"] = 1\n","\n","    df = df.dropna(subset=[\"question\",\"answer\"])\n","    return df[[\"question\",\"answer\",\"evidence\",\"labels\"]]\n","\n","# ---------- Simple paraphrasing (template-based) ----------\n","Q_TEMPLATES = [\n","    \"Is it true that {q}\",\n","    \"Consider the statement: {q}\",\n","    \"Please evaluate the following claim: {q}\",\n","    \"Assess the truth of this claim: {q}\",\n","    \"According to common knowledge, {q}\",\n","    \"True or false: {q}\",\n","    \"Verify this statement: {q}\",\n","    \"Does the following hold? {q}\",\n","]\n","A_TEMPLATES = [\n","    \"{a}\",\n","    \"In short, {a}\",\n","    \"Answer: {a}\",\n","    \"The correct answer is: {a}\",\n","    \"Briefly: {a}\",\n","]\n","\n","def paraphrase(q, a, rng: random.Random):\n","    q_new = rng.choice(Q_TEMPLATES).format(q=q)\n","    a_new = rng.choice(A_TEMPLATES).format(a=a)\n","    return clean_text(q_new), clean_text(a_new)\n","\n","# ---------- Build FEVER 5k ----------\n","fever_df_full = load_fever_jsonl(INPUT_FEVER)\n","if len(fever_df_full) < 5000:\n","    raise ValueError(f\"FEVER usable rows: {len(fever_df_full)}; need at least 5000.\")\n","fever_5k = fever_df_full.sample(n=5000, random_state=42).reset_index(drop=True)\n","\n","# ---------- Build TruthfulQA 5k (paraphrased) ----------\n","tqa_df = load_truthfulqa(INPUT_TQA)\n","rng = random.Random(42)\n","tqa_rows = []\n","\n","if len(tqa_df) >= 5000:\n","    base = tqa_df.sample(n=5000, random_state=42).reset_index(drop=True)\n","else:\n","    # repeat rows with different paraphrases to reach 5k\n","    repeats = int(np.ceil(5000 / max(1, len(tqa_df))))\n","    base = pd.concat([tqa_df] * repeats, ignore_index=True).sample(n=5000, random_state=42).reset_index(drop=True)\n","\n","for _, row in base.iterrows():\n","    q_p, a_p = paraphrase(row[\"question\"], row[\"answer\"], rng)\n","    tqa_rows.append({\"question\": q_p, \"answer\": a_p, \"evidence\": \"\", \"labels\": int(row[\"labels\"])})\n","\n","tqa_5k_paraphrased = pd.DataFrame(tqa_rows)\n","\n","# ---------- Save individual outputs ----------\n","fever_5k.to_json(OUT_FEVER_5K, orient=\"records\", force_ascii=False, indent=2)\n","tqa_5k_paraphrased.to_json(OUT_TQA_5K, orient=\"records\", force_ascii=False, indent=2)\n","\n","# ---------- Merge and save ----------\n","merged = pd.concat([fever_5k, tqa_5k_paraphrased], ignore_index=True)\n","merged.to_json(OUT_MERGED, orient=\"records\", force_ascii=False, indent=2)\n","\n","# ---------- Summary ----------\n","print(\"Saved files:\")\n","print(f\"- FEVER 5k: {OUT_FEVER_5K}\")\n","print(f\"- TruthfulQA 5k paraphrased: {OUT_TQA_5K}\")\n","print(f\"- MERGED 10k: {OUT_MERGED}\\n\")\n","\n","print(\"Label counts:\")\n","print(\"  FEVER 5k:\\n\", fever_5k[\"labels\"].value_counts())\n","print(\"  TQA 5k paraphrased:\\n\", tqa_5k_paraphrased[\"labels\"].value_counts())\n","print(\"  Merged 10k:\\n\", merged[\"labels\"].value_counts())\n","\n","# OPTIONAL: also write a smaller gzip for easy downloading (keeps Drive light)\n","import gzip, io\n","gz_path = OUT_DIR / \"merged_10k.json.gz\"\n","with gzip.open(gz_path, \"wt\", encoding=\"utf-8\") as gz:\n","    gz.write(merged.to_json(orient=\"records\", force_ascii=False))\n","print(f\"\\nAlso wrote gzipped copy: {gz_path}\")\n"]},{"cell_type":"code","source":["# =========================\n","# Balance to 10k (5k ones, 5k zeros)\n","# =========================\n","import pandas as pd\n","import numpy as np\n","from pathlib import Path\n","import random, json\n","\n","random.seed(42)\n","\n","BASE_DIR = Path(\"/content/drive/MyDrive/Colab Notebooks/merged_data\")\n","FEVER_5K = BASE_DIR / \"fever_5000.json\"\n","TQA_5K   = BASE_DIR / \"truthfulqa_5000_paraphrased.json\"\n","\n","OUT_BALANCED = BASE_DIR / \"balanced_10k.json\"\n","\n","def load_json(path: Path) -> pd.DataFrame:\n","    return pd.read_json(path)\n","\n","fever = load_json(FEVER_5K)\n","tqa   = load_json(TQA_5K)\n","\n","# Sanity\n","for dfname, df in [(\"FEVER\", fever), (\"TQA\", tqa)]:\n","    assert set(df.columns) >= {\"question\",\"answer\",\"evidence\",\"labels\"}, f\"{dfname} bad schema: {df.columns}\"\n","\n","# Split FEVER by label\n","fever_pos = fever[fever[\"labels\"] == 1].copy()\n","fever_neg = fever[fever[\"labels\"] == 0].copy()\n","\n","print(\"FEVER label counts:\\n\", fever[\"labels\"].value_counts())\n","print(\"TQA label counts:\\n\", tqa[\"labels\"].value_counts())\n","\n","# === Target counts ===\n","TARGET_POS = 5000\n","TARGET_NEG = 5000\n","\n","# We'll take all TQA as positives (they're all 1s per your output).\n","tqa_pos = tqa.copy()\n","assert len(tqa_pos) >= TARGET_POS, \"Need at least 5k positives from TQA.\"\n","\n","# ---- Build POSITIVE set (choose exactly 5k) ----\n","# Option A: use exactly the 5k TQA positives (simple & consistent)\n","pos_final = tqa_pos.sample(n=TARGET_POS, random_state=42).reset_index(drop=True)\n","\n","# ---- Build NEGATIVE set (need 5k total) ----\n","neg_needed = TARGET_NEG\n","\n","# 1) Use all FEVER negatives we have\n","neg_parts = []\n","neg_parts.append(fever_neg)\n","neg_needed -= len(fever_neg)\n","\n","print(f\"FEVER negatives used: {len(fever_neg)}, still need: {neg_needed}\")\n","\n","# 2) Create synthetic TQA negatives by mismatching answers across questions\n","#    (pair each question with a different answer; label = 0)\n","if neg_needed > 0:\n","    tqa_for_negs = tqa_pos.copy().reset_index(drop=True)\n","    q = tqa_for_negs[\"question\"].tolist()\n","    a = tqa_for_negs[\"answer\"].tolist()\n","\n","    # Create a derangement-like shuffle to avoid pairing question with its own answer\n","    idx = list(range(len(a)))\n","    for _ in range(5):  # a few tries to avoid fixed points\n","        random.shuffle(idx)\n","    # Fix any accidental matches\n","    for i in range(len(idx)):\n","        if idx[i] == i:\n","            j = (i + 1) % len(idx)\n","            idx[i], idx[j] = idx[j], idx[i]\n","\n","    # Build all mismatched pairs\n","    tqa_neg_all = pd.DataFrame({\n","        \"question\": q,\n","        \"answer\": [a[j] for j in idx],\n","        \"evidence\": [\"\" for _ in q],\n","        \"labels\": [0 for _ in q],\n","    })\n","\n","    # Sample exactly the number we need\n","    if len(tqa_neg_all) < neg_needed:\n","        # If somehow not enough, repeat with a different shuffle\n","        reps = int(np.ceil(neg_needed / len(tqa_neg_all)))\n","        tqa_neg_all = pd.concat([tqa_neg_all]*reps, ignore_index=True)\n","\n","    tqa_neg_sample = tqa_neg_all.sample(n=neg_needed, random_state=42).reset_index(drop=True)\n","    neg_parts.append(tqa_neg_sample)\n","\n","neg_final = pd.concat(neg_parts, ignore_index=True)\n","\n","# ---- Assert sizes and labels ----\n","assert len(pos_final) == TARGET_POS, f\"pos_final != {TARGET_POS}\"\n","assert len(neg_final) == TARGET_NEG, f\"neg_final != {TARGET_NEG}\"\n","assert set(pos_final[\"labels\"].unique()) == {1}\n","assert set(neg_final[\"labels\"].unique()) == {0}\n","\n","# ---- Merge, shuffle, save ----\n","balanced = pd.concat([pos_final, neg_final], ignore_index=True)\n","balanced = balanced.sample(frac=1.0, random_state=42).reset_index(drop=True)\n","\n","print(\"Balanced counts:\\n\", balanced[\"labels\"].value_counts())\n","\n","balanced.to_json(OUT_BALANCED, orient=\"records\", force_ascii=False, indent=2)\n","print(f\"\\nSaved balanced dataset to:\\n{OUT_BALANCED}\")\n","\n","# Optional: also save a gzipped version for easy downloading\n","import gzip\n","gz_path = BASE_DIR / \"balanced_10k.json.gz\"\n","with gzip.open(gz_path, \"wt\", encoding=\"utf-8\") as gz:\n","    gz.write(balanced.to_json(orient=\"records\", force_ascii=False))\n","print(f\"Also wrote gzipped copy: {gz_path}\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eIlOTb1Ma6Nx","executionInfo":{"status":"ok","timestamp":1754669016776,"user_tz":0,"elapsed":300,"user":{"displayName":"Bruce Jnr","userId":"02572787615479852412"}},"outputId":"a7cf647e-085f-4a0a-d67b-9ca7ed32a8e8"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["FEVER label counts:\n"," labels\n","1    3605\n","0    1395\n","Name: count, dtype: int64\n","TQA label counts:\n"," labels\n","1    5000\n","Name: count, dtype: int64\n","FEVER negatives used: 1395, still need: 3605\n","Balanced counts:\n"," labels\n","0    5000\n","1    5000\n","Name: count, dtype: int64\n","\n","Saved balanced dataset to:\n","/content/drive/MyDrive/Colab Notebooks/merged_data/balanced_10k.json\n","Also wrote gzipped copy: /content/drive/MyDrive/Colab Notebooks/merged_data/balanced_10k.json.gz\n"]}]},{"cell_type":"code","source":["# =========================\n","# Split balanced_10k.json into 70%/15%/15%\n","# =========================\n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","from pathlib import Path\n","import gzip\n","\n","BASE_DIR = Path(\"/content/drive/MyDrive/Colab Notebooks/merged_data\")\n","BALANCED_PATH = BASE_DIR / \"balanced_10k.json\"\n","\n","OUT_TRAIN = BASE_DIR / \"train_70.json\"\n","OUT_VAL   = BASE_DIR / \"val_15.json\"\n","OUT_TEST  = BASE_DIR / \"test_15.json\"\n","\n","# Load balanced dataset\n","df = pd.read_json(BALANCED_PATH)\n","\n","print(\"Full dataset counts:\\n\", df[\"labels\"].value_counts())\n","\n","# First split: train vs temp (val+test)\n","train_df, temp_df = train_test_split(\n","    df,\n","    test_size=0.30,  # 30% goes to val+test\n","    stratify=df[\"labels\"],\n","    random_state=42\n",")\n","\n","# Second split: val vs test from the temp set\n","val_df, test_df = train_test_split(\n","    temp_df,\n","    test_size=0.50,  # half of temp = 15% of original\n","    stratify=temp_df[\"labels\"],\n","    random_state=42\n",")\n","\n","# Sanity checks\n","print(\"\\nTrain counts:\\n\", train_df[\"labels\"].value_counts())\n","print(\"Val counts:\\n\", val_df[\"labels\"].value_counts())\n","print(\"Test counts:\\n\", test_df[\"labels\"].value_counts())\n","\n","# Save JSON\n","train_df.to_json(OUT_TRAIN, orient=\"records\", force_ascii=False, indent=2)\n","val_df.to_json(OUT_VAL, orient=\"records\", force_ascii=False, indent=2)\n","test_df.to_json(OUT_TEST, orient=\"records\", force_ascii=False, indent=2)\n","\n","# Also save gzipped versions for quicker download\n","for path in [OUT_TRAIN, OUT_VAL, OUT_TEST]:\n","    gz_path = path.with_suffix(path.suffix + \".gz\")\n","    with gzip.open(gz_path, \"wt\", encoding=\"utf-8\") as gz:\n","        gz.write(path.read_text(encoding=\"utf-8\"))\n","    print(f\"Gzipped copy: {gz_path}\")\n","\n","print(\"\\nSaved splits to Drive:\")\n","print(f\"- Train: {OUT_TRAIN} ({len(train_df)} rows)\")\n","print(f\"- Val:   {OUT_VAL} ({len(val_df)} rows)\")\n","print(f\"- Test:  {OUT_TEST} ({len(test_df)} rows)\")\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IIGJDQZxb7cT","executionInfo":{"status":"ok","timestamp":1754669243683,"user_tz":0,"elapsed":2965,"user":{"displayName":"Bruce Jnr","userId":"02572787615479852412"}},"outputId":"59d6d201-6416-4136-c9a5-f981cc6af948"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Full dataset counts:\n"," labels\n","0    5000\n","1    5000\n","Name: count, dtype: int64\n","\n","Train counts:\n"," labels\n","1    3500\n","0    3500\n","Name: count, dtype: int64\n","Val counts:\n"," labels\n","1    750\n","0    750\n","Name: count, dtype: int64\n","Test counts:\n"," labels\n","0    750\n","1    750\n","Name: count, dtype: int64\n","Gzipped copy: /content/drive/MyDrive/Colab Notebooks/merged_data/train_70.json.gz\n","Gzipped copy: /content/drive/MyDrive/Colab Notebooks/merged_data/val_15.json.gz\n","Gzipped copy: /content/drive/MyDrive/Colab Notebooks/merged_data/test_15.json.gz\n","\n","Saved splits to Drive:\n","- Train: /content/drive/MyDrive/Colab Notebooks/merged_data/train_70.json (7000 rows)\n","- Val:   /content/drive/MyDrive/Colab Notebooks/merged_data/val_15.json (1500 rows)\n","- Test:  /content/drive/MyDrive/Colab Notebooks/merged_data/test_15.json (1500 rows)\n"]}]},{"cell_type":"code","source":["# ============================================\n","# Colab: Fetch FEVER + TruthfulQA and prepare dataset\n","# ============================================\n","!pip install -q datasets pandas numpy openpyxl wikipedia-api nltk tqdm\n","\n","import json, random, re\n","from pathlib import Path\n","from collections import defaultdict\n","\n","import numpy as np\n","import pandas as pd\n","from tqdm.auto import tqdm\n","from datasets import load_dataset # Removed load_from_disk\n","\n","import nltk\n","nltk.download('punkt', quiet=True)\n","from nltk.tokenize import sent_tokenize\n","\n","import wikipediaapi\n","\n","random.seed(42)\n","\n","# ---------- CONFIG ----------\n","OUT_DIR = Path(\"/content/drive/MyDrive/Colab Notebooks/merged_data_new\")\n","OUT_DIR.mkdir(parents=True, exist_ok=True)\n","\n","N_FEVER = 5000\n","N_TQA   = 5000\n","DROP_NEI = True\n","USE_WIKI = True           # Set False to skip slow Wikipedia lookups\n","LANG = \"en\"               # Wikipedia language\n","\n","# Final split ratios\n","TRAIN_PCT, VAL_PCT, TEST_PCT = 0.70, 0.15, 0.15\n","\n","# ---------- Helpers ----------\n","def clean_text(x):\n","    s = str(x) if x is not None else \"\"\n","    s = s.strip()\n","    s = re.sub(r\"\\s+\", \" \", s)\n","    return s\n","\n","Q_TEMPLATES = [\n","    \"Is it true that {q}\",\n","    \"Consider the statement: {q}\",\n","    \"Please evaluate the following claim: {q}\",\n","    \"Assess the truth of this claim: {q}\",\n","    \"According to common knowledge, {q}\",\n","    \"True or false: {q}\",\n","    \"Verify this statement: {q}\",\n","    \"Does the following hold? {q}\",\n","]\n","A_TEMPLATES = [\n","    \"{a}\",\n","    \"In short, {a}\",\n","    \"Answer: {a}\",\n","    \"The correct answer is: {a}\",\n","    \"Briefly: {a}\",\n","]\n","\n","def paraphrase(q, a, rng):\n","    return clean_text(rng.choice(Q_TEMPLATES).format(q=q)), clean_text(rng.choice(A_TEMPLATES).format(a=a))\n","\n","# ---------- Load FEVER ----------\n","# Load FEVER from Hugging Face Hub directly\n","try:\n","    fever = load_dataset(\"fever\", split=\"train\")  # Attempt to load directly\n","except Exception as e:\n","    print(f\"Error loading FEVER from Hugging Face Hub: {e}\")\n","    # You might need to provide a more specific path or method depending on the dataset's structure on the Hub.\n","    # If the dataset is not publicly available in this manner, you might need to download it manually\n","    # or find a different source.\n","    raise e # Re-raise the exception if loading fails\n","\n","\n","# Fields typical: 'id','label','claim','evidence','verifiable'\n","\n","def map_fever_label(lbl):\n","    s = str(lbl).strip().upper()\n","    if s == \"SUPPORTS\": return 1\n","    if s == \"REFUTES\":  return 0\n","    return None  # NEI/unknown\n","\n","# Build a small index of FEVER rows by label (drop NEI if configured)\n","fever_rows = []\n","for ex in fever:\n","    y = map_fever_label(ex[\"label\"])\n","    if y is None and DROP_NEI:\n","        continue\n","    fever_rows.append({\n","        \"claim\": clean_text(ex[\"claim\"]),\n","        \"evidence_field\": ex.get(\"evidence\", None),  # nested annotations (page, sentence id)\n","        \"label\": y\n","    })\n","\n","fever_df = pd.DataFrame(fever_rows)\n","if DROP_NEI:\n","    fever_df = fever_df.dropna(subset=[\"label\"]).copy()\n","fever_df[\"label\"] = fever_df[\"label\"].astype(int)\n","\n","# Balance SUPPORTS/REFUTES if possible\n","counts = fever_df[\"label\"].value_counts().to_dict()\n","n_per_class = min(counts.get(0, 0), counts.get(1, 0), N_FEVER // 2)\n","if n_per_class * 2 < N_FEVER:\n","    # if we can't perfectly balance because one class is too small, we fill the rest from the larger class\n","    n0 = min(counts.get(0, 0), N_FEVER // 2)\n","    n1 = min(counts.get(1, 0), N_FEVER - n0)\n","else:\n","    n0 = n1 = n_per_class\n","\n","fever_0 = fever_df[fever_df[\"label\"] == 0].sample(n=n0, random_state=42)\n","fever_1 = fever_df[fever_df[\"label\"] == 1].sample(n=n1, random_state=42)\n","fever_sample = pd.concat([fever_0, fever_1], ignore_index=True)\n","if len(fever_sample) < N_FEVER:\n","    # top up with remaining examples from the larger pool\n","    remainder = N_FEVER - len(fever_sample)\n","    pool = fever_df.drop(fever_sample.index)\n","    fever_sample = pd.concat([fever_sample, pool.sample(n=remainder, random_state=42)], ignore_index=True)\n","\n","fever_sample = fever_sample.sample(frac=1.0, random_state=42).reset_index(drop=True)\n","\n","# ---------- Wikipedia evidence extraction (best-effort) ----------\n","def extract_evidence_sentences(fever_row_list, lang=\"en\"):\n","    \"\"\"\n","    For each FEVER row, try to pull one evidence sentence from Wikipedia based on the first\n","    evidence annotation: (page_title, sentence_index). Caches pages.\n","    If unavailable, returns empty evidence.\n","    \"\"\"\n","    wiki = wikipediaapi.Wikipedia(lang)\n","    page_cache = {}\n","    out_evidence = []\n","    for row in tqdm(fever_row_list, desc=\"Fetching Wikipedia evidence\"):\n","        ev = row[\"evidence_field\"]\n","        ev_text = \"\"\n","\n","        # FEVER 'evidence' is a list of lists of annotations; we pick the first valid (title, sentence id)\n","        page_title = None\n","        sent_idx = None\n","        if isinstance(ev, list) and len(ev) > 0:\n","            # Find a tuple [annotator_id, page, sentence_id, ...]\n","            found = False\n","            for group in ev:\n","                if isinstance(group, list):\n","                    for annot in group:\n","                        if isinstance(annot, list) and len(annot) >= 3:\n","                            page_title = annot[1]\n","                            sent_idx = annot[2]\n","                            if page_title is not None and isinstance(sent_idx, int):\n","                                found = True\n","                                break\n","                if found:\n","                    break\n","\n","        if page_title:\n","            page_title_clean = str(page_title).replace(\"_\", \" \")\n","            if page_title_clean not in page_cache:\n","                page_cache[page_title_clean] = wiki.page(page_title_clean).text or \"\"\n","            page_text = page_cache.get(page_title_clean, \"\")\n","            if page_text:\n","                sents = sent_tokenize(page_text)\n","                if isinstance(sent_idx, int) and 0 <= sent_idx < len(sents):\n","                    ev_text = sents[sent_idx]\n","                else:\n","                    # fallback: take first sentence mentioning the claim's head noun / title fragment\n","                    found_sent = \"\"\n","                    key = page_title_clean.split(\" \")[0]\n","                    for s in sents[:20]:\n","                        if key.lower() in s.lower():\n","                            found_sent = s\n","                            break\n","                    ev_text = found_sent or (sents[0] if sents else \"\")\n","\n","        out_evidence.append(clean_text(ev_text))\n","\n","    return out_evidence\n","\n","if USE_WIKI:\n","    fever_sample[\"evidence\"] = extract_evidence_sentences(fever_sample.to_dict(\"records\"), lang=LANG)\n","else:\n","    fever_sample[\"evidence\"] = \"\"\n","\n","# Build FEVER unified schema: treat claim as both question & answer (statement verification)\n","fever_ready = pd.DataFrame({\n","    \"question\": fever_sample[\"claim\"].apply(clean_text),\n","    \"answer\":   fever_sample[\"claim\"].apply(clean_text),\n","    \"evidence\": fever_sample[\"evidence\"].apply(clean_text),\n","    \"labels\":   fever_sample[\"label\"].astype(int)\n","})\n","\n","print(\"FEVER ready shape:\", fever_ready.shape, fever_ready[\"labels\"].value_counts().to_dict())\n","\n","# ---------- Load TruthfulQA (generation split) ----------\n","tqa = load_dataset(\"truthful_qa\", \"generation\", split=\"validation\")  # 'validation' has the standard set\n","# fields include: question, best_answer, correct_answers (list), incorrect_answers (list)\n","\n","tqa_rows = []\n","for ex in tqa:\n","    q = clean_text(ex[\"question\"])\n","    best = clean_text(ex.get(\"best_answer\", \"\"))\n","    correct_list = [clean_text(x) for x in ex.get(\"correct_answers\", []) if clean_text(x)]\n","    incorrect_list = [clean_text(x) for x in ex.get(\"incorrect_answers\", []) if clean_text(x)]\n","\n","    if q and best:\n","        # positive\n","        tqa_rows.append({\"question\": q, \"answer\": best, \"labels\": 1})\n","    # make a negative if possible\n","    if q and incorrect_list:\n","        neg_ans = random.choice(incorrect_list)\n","        tqa_rows.append({\"question\": q, \"answer\": neg_ans, \"labels\": 0})\n","\n","tqa_df = pd.DataFrame(tqa_rows).dropna()\n","# Paraphrase to diversify (lightweight)\n","rng = random.Random(42)\n","q_new, a_new, y = [], [], []\n","for _, r in tqa_df.iterrows():\n","    q_p, a_p = paraphrase(r[\"question\"], r[\"answer\"], rng)\n","    q_new.append(q_p); a_new.append(a_p); y.append(int(r[\"labels\"]))\n","tqa_df = pd.DataFrame({\"question\": q_new, \"answer\": a_new, \"labels\": y})\n","tqa_df[\"evidence\"] = \"\"  # no passages available here\n","\n","# Sample exactly N_TQA (balanced-ish if possible)\n","# Try to take half positives, half negatives\n","pos = tqa_df[tqa_df[\"labels\"] == 1]\n","neg = tqa_df[tqa_df[\"labels\"] == 0]\n","n_each = min(len(pos), len(neg), N_TQA // 2)\n","if 2 * n_each < N_TQA:\n","    extra = N_TQA - 2 * n_each\n","    # Fill extra with whichever class has more\n","    extra_df = pos if len(pos) > len(neg) else neg\n","    tqa_sample = pd.concat([\n","        pos.sample(n=n_each, random_state=42),\n","        neg.sample(n=n_each, random_state=42),\n","        extra_df.sample(n=extra, random_state=42)\n","    ], ignore_index=True)\n","else:\n","    tqa_sample = pd.concat([\n","        pos.sample(n=n_each, random_state=42),\n","        neg.sample(n=n_each, random_state=42)\n","    ], ignore_index=True)\n","\n","tqa_sample = tqa_sample.sample(frac=1.0, random_state=42).reset_index(drop=True)\n","tqa_ready = tqa_sample[[\"question\",\"answer\",\"evidence\",\"labels\"]].copy()\n","print(\"TruthfulQA ready shape:\", tqa_ready.shape, tqa_ready[\"labels\"].value_counts().to_dict())\n","\n","# ---------- Merge and save master 10k ----------\n","merged = pd.concat([fever_ready, tqa_ready], ignore_index=True)\n","merged = merged.sample(frac=1.0, random_state=42).reset_index(drop=True)\n","\n","MASTER_PATH = OUT_DIR / \"merged_10k_fever_tqa.json\"\n","merged.to_json(MASTER_PATH, orient=\"records\", force_ascii=False, indent=2)\n","print(f\"Saved merged 10k → {MASTER_PATH}\")\n","\n","# ---------- (Optional) Rebalance merged to 50/50 exactly ----------\n","pos_ct = (merged[\"labels\"] == 1).sum()\n","neg_ct = (merged[\"labels\"] == 0).sum()\n","print(\"Merged label counts:\", {\"1\": int(pos_ct), \"0\": int(neg_ct)})\n","\n","# If you want *exact* 50/50 (5k/5k), do:\n","TARGET_TOTAL = len(merged)\n","TARGET_PER = TARGET_TOTAL // 2\n","pos_df = merged[merged[\"labels\"] == 1]\n","neg_df = merged[merged[\"labels\"] == 0]\n","if len(pos_df) >= TARGET_PER and len(neg_df) >= TARGET_PER:\n","    merged_bal = pd.concat([\n","        pos_df.sample(n=TARGET_PER, random_state=42),\n","        neg_df.sample(n=TARGET_PER, random_state=42)\n","    ], ignore_index=True).sample(frac=1.0, random_state=42).reset_index(drop=True)\n","else:\n","    merged_bal = merged  # fallback if one class too small\n","\n","BALANCED_PATH = OUT_DIR / \"merged_10k_balanced.json\"\n","merged_bal.to_json(BALANCED_PATH, orient=\"records\", force_ascii=False, indent=2)\n","print(f\"Saved balanced 10k → {BALANCED_PATH}\")\n","print(\"Balanced label counts:\", merged_bal[\"labels\"].value_counts().to_dict())\n","\n","# ---------- 70/15/15 stratified split ----------\n","from sklearn.model_selection import train_test_split\n","\n","df = merged_bal.copy()\n","train_df, temp_df = train_test_split(\n","    df, test_size=(1.0 - TRAIN_PCT), stratify=df[\"labels\"], random_state=42\n",")\n","val_df, test_df = train_test_split(\n","    temp_df, test_size=TEST_PCT / (VAL_PCT + TEST_PCT),\n","    stratify=temp_df[\"labels\"], random_state=42\n",")\n","\n","TRAIN_JSON = OUT_DIR / \"train_70.json\"\n","VAL_JSON   = OUT_DIR / \"val_15.json\"\n","TEST_JSON  = OUT_DIR / \"test_15.json\"\n","\n","train_df.to_json(TRAIN_JSON, orient=\"records\", force_ascii=False, indent=2)\n","val_df.to_json(VAL_JSON, orient=\"records\", force_ascii=False, indent=2)\n","test_df.to_json(TEST_JSON, orient=\"records\", force_ascii=False, indent=2)\n","\n","print(\"\\nSaved splits:\")\n","print(f\"- Train ({len(train_df)}): {TRAIN_JSON}\")\n","print(f\"- Val   ({len(val_df)}): {VAL_JSON}\")\n","print(f\"- Test  ({len(test_df)}): {TEST_JSON}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":356},"id":"AMhJZebLjvwl","executionInfo":{"status":"error","timestamp":1754671594731,"user_tz":0,"elapsed":4671,"user":{"displayName":"Bruce Jnr","userId":"02572787615479852412"}},"outputId":"e09205ef-b6e0-4f85-ece8-a8496e491907"},"execution_count":5,"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"Unable to find 'hf://datasets/fever/train.parquet'","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-114756606.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;31m# ============== Load FEVER from Parquet (HF Hub) ==============\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;31m# (Old script-based loader is deprecated; Parquet works)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m fever = load_dataset(\n\u001b[0m\u001b[1;32m     70\u001b[0m     \u001b[0;34m\"parquet\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m     \u001b[0mdata_files\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"hf://datasets/fever/train.parquet\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1391\u001b[0m     \u001b[0;31m# Create a dataset builder\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1392\u001b[0;31m     builder_instance = load_dataset_builder(\n\u001b[0m\u001b[1;32m   1393\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1394\u001b[0m         \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mload_dataset_builder\u001b[0;34m(path, name, data_dir, data_files, cache_dir, features, download_config, download_mode, revision, token, storage_options, **config_kwargs)\u001b[0m\n\u001b[1;32m   1130\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfeatures\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1131\u001b[0m         \u001b[0mfeatures\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_fix_for_backward_compatible_features\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1132\u001b[0;31m     dataset_module = dataset_module_factory(\n\u001b[0m\u001b[1;32m   1133\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1134\u001b[0m         \u001b[0mrevision\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mrevision\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mdataset_module_factory\u001b[0;34m(path, revision, download_config, download_mode, data_dir, data_files, cache_dir, **download_kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m             \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    911\u001b[0m             \u001b[0mdownload_mode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdownload_mode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 912\u001b[0;31m         ).get_module()\n\u001b[0m\u001b[1;32m    913\u001b[0m     \u001b[0;31m# Try locally\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/load.py\u001b[0m in \u001b[0;36mget_module\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0;32melse\u001b[0m \u001b[0mget_data_patterns\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    525\u001b[0m         )\n\u001b[0;32m--> 526\u001b[0;31m         data_files = DataFilesDict.from_patterns(\n\u001b[0m\u001b[1;32m    527\u001b[0m             \u001b[0mpatterns\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    528\u001b[0m             \u001b[0mdownload_config\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdownload_config\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mfrom_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    687\u001b[0m                 \u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mDataFilesList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m                 else DataFilesList.from_patterns(\n\u001b[0m\u001b[1;32m    690\u001b[0m                     \u001b[0mpatterns_for_key\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m                     \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mfrom_patterns\u001b[0;34m(cls, patterns, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    580\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    581\u001b[0m                 data_files.extend(\n\u001b[0;32m--> 582\u001b[0;31m                     resolve_pattern(\n\u001b[0m\u001b[1;32m    583\u001b[0m                         \u001b[0mpattern\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    584\u001b[0m                         \u001b[0mbase_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbase_path\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.11/dist-packages/datasets/data_files.py\u001b[0m in \u001b[0;36mresolve_pattern\u001b[0;34m(pattern, base_path, allowed_extensions, download_config)\u001b[0m\n\u001b[1;32m    381\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mallowed_extensions\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    382\u001b[0m             \u001b[0merror_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" with any supported extension {list(allowed_extensions)}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 383\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mFileNotFoundError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merror_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    384\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: Unable to find 'hf://datasets/fever/train.parquet'"]}]}]}