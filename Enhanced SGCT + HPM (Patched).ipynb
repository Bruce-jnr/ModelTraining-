{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"gpuType":"T4","mount_file_id":"1SwnS58Bb9iICGrRTkNoloLHY1NPA9q04","authorship_tag":"ABX9TyM/66oEKKU2k/pJ1jaEfsE/"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU","widgets":{"application/vnd.jupyter.widget-state+json":{"96dd16647eb544618c7e273324d081d4":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_739690cfdef143b2a36cadfebcc2e659","IPY_MODEL_3b2d5d7143944e53bf0207ed206316df","IPY_MODEL_2331b6fa9e214699a902d01a53332687"],"layout":"IPY_MODEL_7df89f92115c4e0caa1307feb2719d6b"}},"739690cfdef143b2a36cadfebcc2e659":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f4a87d391b4b4aaeba4dca3c150af4bd","placeholder":"​","style":"IPY_MODEL_9ad9557dc5314b399bd5ba9ecf62d563","value":"tokenizer_config.json: 100%"}},"3b2d5d7143944e53bf0207ed206316df":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8e6bcef942a64c39986a2526ff357231","max":26,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ec9dccf116fc49af9b433b3e85682dbd","value":26}},"2331b6fa9e214699a902d01a53332687":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c0cbfe4fa7e44d0584ffd1f0cb4a53ca","placeholder":"​","style":"IPY_MODEL_d52c506aa26e4442aa56ff8ddf6d66f9","value":" 26.0/26.0 [00:00&lt;00:00, 1.48kB/s]"}},"7df89f92115c4e0caa1307feb2719d6b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f4a87d391b4b4aaeba4dca3c150af4bd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9ad9557dc5314b399bd5ba9ecf62d563":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8e6bcef942a64c39986a2526ff357231":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ec9dccf116fc49af9b433b3e85682dbd":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c0cbfe4fa7e44d0584ffd1f0cb4a53ca":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d52c506aa26e4442aa56ff8ddf6d66f9":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c9046d8e147143b78d5fa5ac86dc2046":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_52201f33627846218b1ab5698b72812b","IPY_MODEL_9308f52067184d1582d288dea8a530fc","IPY_MODEL_2995df3979ef411eac864529dee29624"],"layout":"IPY_MODEL_e717835ae00247e6a7104ca360dcf1ee"}},"52201f33627846218b1ab5698b72812b":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_8b538eb4c06b4855bcb8457db978147a","placeholder":"​","style":"IPY_MODEL_08c3867be28c4245b69c167886fdb885","value":"vocab.json: 100%"}},"9308f52067184d1582d288dea8a530fc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4b3312236d4d47ad8b85eb842ab5c116","max":1042301,"min":0,"orientation":"horizontal","style":"IPY_MODEL_09c5fdad200949669b4eb97098182e98","value":1042301}},"2995df3979ef411eac864529dee29624":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_513723f9598647de9c93df9499630e91","placeholder":"​","style":"IPY_MODEL_e2ab1ad21a3c47b6a4fbba65d7ea1907","value":" 1.04M/1.04M [00:00&lt;00:00, 1.59MB/s]"}},"e717835ae00247e6a7104ca360dcf1ee":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"8b538eb4c06b4855bcb8457db978147a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"08c3867be28c4245b69c167886fdb885":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4b3312236d4d47ad8b85eb842ab5c116":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"09c5fdad200949669b4eb97098182e98":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"513723f9598647de9c93df9499630e91":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e2ab1ad21a3c47b6a4fbba65d7ea1907":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ce347edfc31142f385344e573c50a0d9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_b7eff26709e64e339a67f1c168319696","IPY_MODEL_78e668bd587a4ecc9de6e2557d757fcd","IPY_MODEL_9b650686b62840e28f9eb45d4e19b8e2"],"layout":"IPY_MODEL_3275035861e74df99d1c620554fbacdb"}},"b7eff26709e64e339a67f1c168319696":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_95d9bb834d374f6786a392092bfa2e8e","placeholder":"​","style":"IPY_MODEL_a30912d954c24d549f51bc0d597e901d","value":"merges.txt: 100%"}},"78e668bd587a4ecc9de6e2557d757fcd":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_dcb4cbd3b35e459d8b35938e8f124b2d","max":456318,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e9608d39a2354d69b556947fef81572c","value":456318}},"9b650686b62840e28f9eb45d4e19b8e2":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_83bcd7257e444e6dbb365ccbaaa32125","placeholder":"​","style":"IPY_MODEL_f9a20810b06341c9b6d2198ac248a90b","value":" 456k/456k [00:00&lt;00:00, 23.1MB/s]"}},"3275035861e74df99d1c620554fbacdb":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"95d9bb834d374f6786a392092bfa2e8e":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a30912d954c24d549f51bc0d597e901d":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"dcb4cbd3b35e459d8b35938e8f124b2d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e9608d39a2354d69b556947fef81572c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"83bcd7257e444e6dbb365ccbaaa32125":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f9a20810b06341c9b6d2198ac248a90b":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"b8be36cde32a425bb90703cf5efd6029":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_12203d3932d54834b5682e578df693d8","IPY_MODEL_da522ac4e2654f4faf7872f9f4fc53e1","IPY_MODEL_8e6c75892a3f4ed89e7ec2605cf1c6cb"],"layout":"IPY_MODEL_7097bae8bdb546e79d02867ce8f5e6aa"}},"12203d3932d54834b5682e578df693d8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d5027eb282184f38ab56cd37e52c4718","placeholder":"​","style":"IPY_MODEL_f85398112e644d8989abd9dd1d669fa8","value":"tokenizer.json: 100%"}},"da522ac4e2654f4faf7872f9f4fc53e1":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_9691a0ccf1574288adee10223f7109ad","max":1355256,"min":0,"orientation":"horizontal","style":"IPY_MODEL_9da02e54a14447d0be4d45f0f6544c2c","value":1355256}},"8e6c75892a3f4ed89e7ec2605cf1c6cb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_82c375ecd7dd41858d1e62e4a4daeef6","placeholder":"​","style":"IPY_MODEL_21e6289a156847cda65ae482282c9d54","value":" 1.36M/1.36M [00:00&lt;00:00, 3.05MB/s]"}},"7097bae8bdb546e79d02867ce8f5e6aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d5027eb282184f38ab56cd37e52c4718":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f85398112e644d8989abd9dd1d669fa8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"9691a0ccf1574288adee10223f7109ad":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9da02e54a14447d0be4d45f0f6544c2c":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"82c375ecd7dd41858d1e62e4a4daeef6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"21e6289a156847cda65ae482282c9d54":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5abe67965fb24d4b9e3ab4a357cf6443":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_540808ba88334f5ea11629567fba2b18","IPY_MODEL_ea53d3a42930419c8b8470fdb0be5dab","IPY_MODEL_fc7d2daa0396434bbb10c3654845289e"],"layout":"IPY_MODEL_6d5b6bbcb9f649ccae3aae5836558af6"}},"540808ba88334f5ea11629567fba2b18":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c6190593a8f64edd8186a2238a72f061","placeholder":"​","style":"IPY_MODEL_3a29396a13534164aef25648ac5d459c","value":"config.json: 100%"}},"ea53d3a42930419c8b8470fdb0be5dab":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_c2adbdde8e6f4cd8a6e8d721cc10e1bc","max":665,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ded6e39816da45e6a4354eefdfcaf815","value":665}},"fc7d2daa0396434bbb10c3654845289e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3faceefcd9c8415da643c2f8865ea9e3","placeholder":"​","style":"IPY_MODEL_f82eb081fef74dcea44bedc689ff7162","value":" 665/665 [00:00&lt;00:00, 72.5kB/s]"}},"6d5b6bbcb9f649ccae3aae5836558af6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c6190593a8f64edd8186a2238a72f061":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3a29396a13534164aef25648ac5d459c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"c2adbdde8e6f4cd8a6e8d721cc10e1bc":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ded6e39816da45e6a4354eefdfcaf815":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3faceefcd9c8415da643c2f8865ea9e3":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f82eb081fef74dcea44bedc689ff7162":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"55d105a1c1574544b932e66080b148f9":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e7ec9d5aff58487ea4face36d291d20a","IPY_MODEL_09ea24404eef45c5a4ac47e712449567","IPY_MODEL_742b7403c99f4aa2a10e0868c2e48be8"],"layout":"IPY_MODEL_fc7e2aee1c7141d9913d60e1d498da5f"}},"e7ec9d5aff58487ea4face36d291d20a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_560a0b0aa2eb4de29b5630fae434b5c2","placeholder":"​","style":"IPY_MODEL_b6b79f5bcf944363bc805c2d33e1541c","value":"model.safetensors: 100%"}},"09ea24404eef45c5a4ac47e712449567":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_f7495d189d86446f8b6a15a28d89d744","max":548105171,"min":0,"orientation":"horizontal","style":"IPY_MODEL_fe97478205b149c58c89158545bd692a","value":548105171}},"742b7403c99f4aa2a10e0868c2e48be8":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_67c8f32524ab41d98250df123a577e2b","placeholder":"​","style":"IPY_MODEL_68b537ecc0224cbaaf2dfafbfe0cee5c","value":" 548M/548M [00:09&lt;00:00, 54.9MB/s]"}},"fc7e2aee1c7141d9913d60e1d498da5f":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"560a0b0aa2eb4de29b5630fae434b5c2":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b6b79f5bcf944363bc805c2d33e1541c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f7495d189d86446f8b6a15a28d89d744":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"fe97478205b149c58c89158545bd692a":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"67c8f32524ab41d98250df123a577e2b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"68b537ecc0224cbaaf2dfafbfe0cee5c":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5d6f2d3cf6ae4d5783cd2030b0531c65":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_d2d61f4fb7bc4e2b96c164cb023df49a","IPY_MODEL_164df8f8ce8d47dd94c3532b953188a9","IPY_MODEL_3151c184097c4530a71622ff84fd8fc4"],"layout":"IPY_MODEL_c940b2882baf4e819b0c7e8b37e57ff0"}},"d2d61f4fb7bc4e2b96c164cb023df49a":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6f9b851d252742179f7375e4c1f3cfed","placeholder":"​","style":"IPY_MODEL_4562f542dcf14c74bac99a2b13adb1f3","value":"generation_config.json: 100%"}},"164df8f8ce8d47dd94c3532b953188a9":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_69a924d824b44ed7bcfcd505c922aecd","max":124,"min":0,"orientation":"horizontal","style":"IPY_MODEL_2ed12f0dccce4d4bb4d67e225ef6fab6","value":124}},"3151c184097c4530a71622ff84fd8fc4":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3f5a26869cda41c986454e674ab96fd6","placeholder":"​","style":"IPY_MODEL_d4059ab4ca1242f09a6c4fd2a363db2e","value":" 124/124 [00:00&lt;00:00, 3.95kB/s]"}},"c940b2882baf4e819b0c7e8b37e57ff0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6f9b851d252742179f7375e4c1f3cfed":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4562f542dcf14c74bac99a2b13adb1f3":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"69a924d824b44ed7bcfcd505c922aecd":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2ed12f0dccce4d4bb4d67e225ef6fab6":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"3f5a26869cda41c986454e674ab96fd6":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d4059ab4ca1242f09a6c4fd2a363db2e":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"cells":[{"cell_type":"code","execution_count":2,"metadata":{"id":"pHxlISI9WFfb","colab":{"base_uri":"https://localhost:8080/","height":843,"referenced_widgets":["96dd16647eb544618c7e273324d081d4","739690cfdef143b2a36cadfebcc2e659","3b2d5d7143944e53bf0207ed206316df","2331b6fa9e214699a902d01a53332687","7df89f92115c4e0caa1307feb2719d6b","f4a87d391b4b4aaeba4dca3c150af4bd","9ad9557dc5314b399bd5ba9ecf62d563","8e6bcef942a64c39986a2526ff357231","ec9dccf116fc49af9b433b3e85682dbd","c0cbfe4fa7e44d0584ffd1f0cb4a53ca","d52c506aa26e4442aa56ff8ddf6d66f9","c9046d8e147143b78d5fa5ac86dc2046","52201f33627846218b1ab5698b72812b","9308f52067184d1582d288dea8a530fc","2995df3979ef411eac864529dee29624","e717835ae00247e6a7104ca360dcf1ee","8b538eb4c06b4855bcb8457db978147a","08c3867be28c4245b69c167886fdb885","4b3312236d4d47ad8b85eb842ab5c116","09c5fdad200949669b4eb97098182e98","513723f9598647de9c93df9499630e91","e2ab1ad21a3c47b6a4fbba65d7ea1907","ce347edfc31142f385344e573c50a0d9","b7eff26709e64e339a67f1c168319696","78e668bd587a4ecc9de6e2557d757fcd","9b650686b62840e28f9eb45d4e19b8e2","3275035861e74df99d1c620554fbacdb","95d9bb834d374f6786a392092bfa2e8e","a30912d954c24d549f51bc0d597e901d","dcb4cbd3b35e459d8b35938e8f124b2d","e9608d39a2354d69b556947fef81572c","83bcd7257e444e6dbb365ccbaaa32125","f9a20810b06341c9b6d2198ac248a90b","b8be36cde32a425bb90703cf5efd6029","12203d3932d54834b5682e578df693d8","da522ac4e2654f4faf7872f9f4fc53e1","8e6c75892a3f4ed89e7ec2605cf1c6cb","7097bae8bdb546e79d02867ce8f5e6aa","d5027eb282184f38ab56cd37e52c4718","f85398112e644d8989abd9dd1d669fa8","9691a0ccf1574288adee10223f7109ad","9da02e54a14447d0be4d45f0f6544c2c","82c375ecd7dd41858d1e62e4a4daeef6","21e6289a156847cda65ae482282c9d54","5abe67965fb24d4b9e3ab4a357cf6443","540808ba88334f5ea11629567fba2b18","ea53d3a42930419c8b8470fdb0be5dab","fc7d2daa0396434bbb10c3654845289e","6d5b6bbcb9f649ccae3aae5836558af6","c6190593a8f64edd8186a2238a72f061","3a29396a13534164aef25648ac5d459c","c2adbdde8e6f4cd8a6e8d721cc10e1bc","ded6e39816da45e6a4354eefdfcaf815","3faceefcd9c8415da643c2f8865ea9e3","f82eb081fef74dcea44bedc689ff7162","55d105a1c1574544b932e66080b148f9","e7ec9d5aff58487ea4face36d291d20a","09ea24404eef45c5a4ac47e712449567","742b7403c99f4aa2a10e0868c2e48be8","fc7e2aee1c7141d9913d60e1d498da5f","560a0b0aa2eb4de29b5630fae434b5c2","b6b79f5bcf944363bc805c2d33e1541c","f7495d189d86446f8b6a15a28d89d744","fe97478205b149c58c89158545bd692a","67c8f32524ab41d98250df123a577e2b","68b537ecc0224cbaaf2dfafbfe0cee5c","5d6f2d3cf6ae4d5783cd2030b0531c65","d2d61f4fb7bc4e2b96c164cb023df49a","164df8f8ce8d47dd94c3532b953188a9","3151c184097c4530a71622ff84fd8fc4","c940b2882baf4e819b0c7e8b37e57ff0","6f9b851d252742179f7375e4c1f3cfed","4562f542dcf14c74bac99a2b13adb1f3","69a924d824b44ed7bcfcd505c922aecd","2ed12f0dccce4d4bb4d67e225ef6fab6","3f5a26869cda41c986454e674ab96fd6","d4059ab4ca1242f09a6c4fd2a363db2e"]},"executionInfo":{"status":"error","timestamp":1756907475926,"user_tz":0,"elapsed":639702,"user":{"displayName":"Bruce Jnr","userId":"02572787615479852412"}},"outputId":"a06a5fe3-f30e-4652-96be-ed37c5726b36"},"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.1/40.1 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.6/11.6 MB\u001b[0m \u001b[31m115.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m9.5/9.5 MB\u001b[0m \u001b[31m42.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.7/8.7 MB\u001b[0m \u001b[31m53.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.3/3.3 MB\u001b[0m \u001b[31m113.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDrive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n","The secret `HF_TOKEN` does not exist in your Colab secrets.\n","To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n","You will be able to reuse this secret in all of your notebooks.\n","Please note that authentication is recommended but still optional to access public models or datasets.\n","  warnings.warn(\n"]},{"output_type":"display_data","data":{"text/plain":["tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"96dd16647eb544618c7e273324d081d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c9046d8e147143b78d5fa5ac86dc2046"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce347edfc31142f385344e573c50a0d9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8be36cde32a425bb90703cf5efd6029"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["config.json:   0%|          | 0.00/665 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5abe67965fb24d4b9e3ab4a357cf6443"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["model.safetensors:   0%|          | 0.00/548M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55d105a1c1574544b932e66080b148f9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5d6f2d3cf6ae4d5783cd2030b0531c65"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Eval (pre/post): 100%|██████████| 487/487 [07:36<00:00,  1.07it/s]\n","Building contrastive pairs:   0%|          | 7/2270 [00:46<4:13:11,  6.71s/it]\n"]},{"output_type":"error","ename":"KeyboardInterrupt","evalue":"","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-3825318166.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    523\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m \u001b[0;31m# ===== Build SGCT contrastive pairs (train split only) =====\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 525\u001b[0;31m \u001b[0mcontrastive_pairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msgct\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuild_pairs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    526\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontrastive_pairs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    527\u001b[0m     \u001b[0mlogger\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"No contrastive pairs produced. Check thresholds or generation.\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3825318166.py\u001b[0m in \u001b[0;36mbuild_pairs\u001b[0;34m(self, qs, gts)\u001b[0m\n\u001b[1;32m    262\u001b[0m         \u001b[0mpairs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    263\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtqdm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mqs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgts\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdesc\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Building contrastive pairs\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 264\u001b[0;31m             \u001b[0mcands\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgenerate_diverse\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    265\u001b[0m             \u001b[0mfilt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhpm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilter_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    266\u001b[0m             \u001b[0mfactual_anchor\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3825318166.py\u001b[0m in \u001b[0;36mgenerate_diverse\u001b[0;34m(self, question, n)\u001b[0m\n\u001b[1;32m    194\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcands\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    195\u001b[0m                 \u001b[0mret\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcands\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sample_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_beams\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_return_sequences\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mret\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    197\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mo\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mouts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m                     \u001b[0mans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_decode_answer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtok\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mo\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mskip_special_tokens\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprompt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/tmp/ipython-input-3825318166.py\u001b[0m in \u001b[0;36m_sample_once\u001b[0;34m(self, enc, **kwargs)\u001b[0m\n\u001b[1;32m    142\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mno_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    143\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_sample_once\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menc\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 144\u001b[0;31m         return self.model.generate(\n\u001b[0m\u001b[1;32m    145\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"input_ids\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m             \u001b[0mattention_mask\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0menc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"attention_mask\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/utils/_contextlib.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    118\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mctx_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36mgenerate\u001b[0;34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, use_model_defaults, custom_generate, **kwargs)\u001b[0m\n\u001b[1;32m   2549\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0mgeneration_mode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBEAM_SAMPLE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mGenerationMode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mBEAM_SEARCH\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2550\u001b[0m             \u001b[0;31m# 11. run beam sample\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2551\u001b[0;31m             result = self._beam_search(\n\u001b[0m\u001b[1;32m   2552\u001b[0m                 \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2553\u001b[0m                 \u001b[0mlogits_processor\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprepared_logits_processor\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/generation/utils.py\u001b[0m in \u001b[0;36m_beam_search\u001b[0;34m(self, input_ids, logits_processor, stopping_criteria, generation_config, synced_gpus, **model_kwargs)\u001b[0m\n\u001b[1;32m   3348\u001b[0m             \u001b[0mmodel_inputs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"output_hidden_states\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0moutput_hidden_states\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0moutput_hidden_states\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3349\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3350\u001b[0;31m             \u001b[0mmodel_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mmodel_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreturn_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3351\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3352\u001b[0m             \u001b[0;31m# synced_gpus: don't waste resources running the code we don't need; kwargs must be updated before skipping\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, labels, use_cache, output_attentions, output_hidden_states, return_dict, logits_to_keep, **kwargs)\u001b[0m\n\u001b[1;32m   1068\u001b[0m         \u001b[0mreturn_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mreturn_dict\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_return_dict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1069\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1070\u001b[0;31m         transformer_outputs = self.transformer(\n\u001b[0m\u001b[1;32m   1071\u001b[0m             \u001b[0minput_ids\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1072\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input_ids, past_key_values, cache_position, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, output_hidden_states, return_dict, **kwargs)\u001b[0m\n\u001b[1;32m    925\u001b[0m                 \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mall_hidden_states\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    926\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 927\u001b[0;31m             outputs = block(\n\u001b[0m\u001b[1;32m    928\u001b[0m                 \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    929\u001b[0m                 \u001b[0mpast_key_values\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient_checkpointing\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtraining\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/modeling_layers.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gradient_checkpointing_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpartial\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 94\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     95\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, past_key_values, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, use_cache, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    412\u001b[0m         \u001b[0mresidual\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhidden_states\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    413\u001b[0m         \u001b[0mhidden_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mln_1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 414\u001b[0;31m         attn_output, self_attn_weights = self.attn(\n\u001b[0m\u001b[1;32m    415\u001b[0m             \u001b[0mhidden_states\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    416\u001b[0m             \u001b[0mpast_key_values\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mpast_key_values\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/utils/deprecation.py\u001b[0m in \u001b[0;36mwrapped_func\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    170\u001b[0m                 \u001b[0mwarnings\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mFutureWarning\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstacklevel\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    171\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 172\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    173\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    174\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mwrapped_func\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/models/gpt2/modeling_gpt2.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, hidden_states, past_key_values, cache_position, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, output_attentions, **kwargs)\u001b[0m\n\u001b[1;32m    312\u001b[0m                 \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvalue_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_kv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 314\u001b[0;31m             \u001b[0mquery_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mc_attn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhidden_states\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdim\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    315\u001b[0m             \u001b[0mshape_kv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead_dim\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    316\u001b[0m             \u001b[0mkey_states\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkey_states\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mshape_kv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1771\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_compiled_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[misc]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1772\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1773\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1774\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1775\u001b[0m     \u001b[0;31m# torchrec tests the code consistency with the following code\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1782\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1783\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1784\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1785\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1786\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.12/dist-packages/transformers/pytorch_utils.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    120\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    121\u001b[0m         \u001b[0msize_out\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 122\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maddmm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    123\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msize_out\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# ==============================================\n","# Enhanced SGCT + HPM (Patched)\n","# ==============================================\n","# Key fixes & improvements:\n","# 1) Evaluation now compares HPM(label(gt_answer)) vs HPM(label(generated)) → no more precision=1.0 artifact.\n","# 2) Collator returns a boolean mask for rows that truly have hallucinated text; UL/contrastive are skipped otherwise.\n","# 3) Contrastive loss replaced with a cosine-margin push-away objective (stable, informative gradients).\n","# 4) Unlikelihood loss applied only on high-probability tokens to reduce noise.\n","# 5) Generation hygiene: no_repeat_ngram_size, repetition_penalty; MAX_LENGTH raised to 256.\n","# 6) build_pairs balances factual/hal pairs per question.\n","# 7) (Optional) HPM threshold sweep utility to suggest a better FACTUAL_THRESHOLD from validation.\n","\n","!pip -q install -U transformers datasets scikit-learn python-dotenv tqdm matplotlib torchaudio sentence-transformers\n","\n","import os, json, warnings, re, random, logging\n","from pathlib import Path\n","from typing import Any, Dict, List, Tuple, Optional\n","from collections import defaultdict\n","\n","import numpy as np\n","import pandas as pd\n","import torch\n","import torch.nn.functional as F\n","from torch.utils.data import Dataset, DataLoader\n","from sklearn.model_selection import train_test_split\n","from sklearn.metrics import accuracy_score, precision_recall_fscore_support, roc_auc_score\n","import matplotlib.pyplot as plt\n","from tqdm import tqdm\n","\n","from transformers import (\n","    DistilBertTokenizerFast,\n","    DistilBertForSequenceClassification,\n","    GPT2LMHeadModel,\n","    GPT2TokenizerFast,\n","    get_linear_schedule_with_warmup,\n",")\n","\n","# =========================\n","# Configuration\n","# =========================\n","class Config:\n","    # Paths\n","    HPM_MODEL_PATH = \"/content/drive/MyDrive/Colab Notebooks/NewBestModel/hallucination_detector_final\"\n","    DATASET_PATH   = \"/content/drive/MyDrive/Colab Notebooks/train_dataset.json\"\n","\n","    # SGCT model + outputs\n","    SGCT_MODEL_NAME = \"gpt2\"  # \"gpt2-medium\" if GPU allows\n","    SGCT_CHECKPOINT_DIR = \"/content/drive/MyDrive/Colab Notebooks/SGCT_checkpoints\"\n","    SGCT_FINAL_DIR      = \"/content/drive/MyDrive/Colab Notebooks/SGCT_final_model\"\n","\n","    # Training\n","    BATCH_SIZE = 4\n","    LEARNING_RATE = 5e-5\n","    SGCT_EPOCHS = 5\n","    PATIENCE = 3\n","    MAX_LENGTH = 256\n","    N_CANDIDATES = 8\n","\n","    # Loss weights (initial)\n","    ALPHA_LIKELIHOOD = 1.0\n","    BETA_UNLIKELIHOOD = 0.1\n","    GAMMA_CONTRASTIVE = 0.2\n","\n","    # HPM thresholds (will be tuned optionally)\n","    FACTUAL_THRESHOLD = 0.7\n","    HALLUCINATED_THRESHOLD = 0.3\n","\n","    # Eval generation\n","    GEN_MAX_NEW_TOKENS = 96\n","    GEN_TOP_P = 0.92\n","    GEN_TOP_K = 50\n","    GEN_TEMPERATURE = 0.8\n","\n","    # Device\n","    DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","\n","config = Config()\n","\n","# =========================\n","# Mount Drive & Logging\n","# =========================\n","from google.colab import drive\n","drive.mount('/content/drive')\n","\n","logging.basicConfig(level=logging.INFO)\n","logger = logging.getLogger(\"SGCT_Enhanced\")\n","\n","# =========================\n","# HPM Filter (DistilBERT classifier)\n","# =========================\n","class HPMFilter:\n","    def __init__(self, model_path: str):\n","        self.device = config.DEVICE\n","        self.tokenizer = DistilBertTokenizerFast.from_pretrained(model_path)\n","        self.model = DistilBertForSequenceClassification.from_pretrained(model_path).to(self.device)\n","        self.model.eval()\n","        logger.info(\"HPM loaded.\")\n","\n","    @torch.no_grad()\n","    def predict_batch(self, questions: List[str], answers: List[str]) -> List[float]:\n","        enc = self.tokenizer(\n","            [f\"Q: {q}\" for q in questions],\n","            answers,\n","            truncation=True, max_length=256, padding=True, return_tensors=\"pt\"\n","        )\n","        out = self.model(input_ids=enc[\"input_ids\"].to(self.device),\n","                         attention_mask=enc[\"attention_mask\"].to(self.device))\n","        probs = F.softmax(out.logits, dim=-1)[:, 1]  # factual prob\n","        return probs.detach().cpu().tolist()\n","\n","    def score(self, q: str, a: str) -> float:\n","        return self.predict_batch([q], [a])[0]\n","\n","    def filter_candidates(self, question: str, candidates: List[str]) -> Dict[str, List]:\n","        if not candidates:\n","            return {\"factual\": [], \"hallucinated\": [], \"uncertain\": [], \"scores\": []}\n","        scores = self.predict_batch([question]*len(candidates), candidates)\n","        factual, hallucinated, uncertain = [], [], []\n","        for c, s in zip(candidates, scores):\n","            if s >= config.FACTUAL_THRESHOLD: factual.append(c)\n","            elif s <= config.HALLUCINATED_THRESHOLD: hallucinated.append(c)\n","            else: uncertain.append(c)\n","        return {\"factual\": factual, \"hallucinated\": hallucinated, \"uncertain\": uncertain, \"scores\": scores}\n","\n","# =========================\n","# Candidate Generator (GPT-2) + Perturbations\n","# =========================\n","class CandidateGenerator:\n","    def __init__(self, model_name=config.SGCT_MODEL_NAME):\n","        self.device = config.DEVICE\n","        self.tok = GPT2TokenizerFast.from_pretrained(model_name)\n","        self.model = GPT2LMHeadModel.from_pretrained(model_name).to(self.device)\n","        if self.tok.pad_token is None:\n","            self.tok.pad_token = self.tok.eos_token\n","            self.model.config.pad_token_id = self.tok.eos_token_id\n","\n","    def _decode_answer(self, full_text: str, prompt: str) -> Optional[str]:\n","        ans = full_text[len(prompt):].strip()\n","        ans = ans.split(\"\\n\")[0].strip()\n","        return ans if len(ans.split()) >= 5 else None\n","\n","    @torch.no_grad()\n","    def _sample_once(self, enc, **kwargs):\n","        return self.model.generate(\n","            input_ids=enc[\"input_ids\"],\n","            attention_mask=enc[\"attention_mask\"],\n","            max_new_tokens=config.GEN_MAX_NEW_TOKENS,\n","            pad_token_id=self.tok.eos_token_id,\n","            no_repeat_ngram_size=3,\n","            repetition_penalty=1.1,\n","            **kwargs\n","        )\n","\n","    # ---- lightweight perturbations to create harder counterfactuals ----\n","    def _negate(self, s: str) -> str:\n","        return re.sub(r\"\\bis\\b\", \"is not\", s, count=1) if \" is \" in s else s + \" not.\"\n","\n","    def _swap_numbers(self, s: str) -> str:\n","        return re.sub(r\"\\b(\\d{1,4})\\b\", lambda m: str(int(m.group(1))+1), s, count=1)\n","\n","    def _swap_entities(self, s: str) -> str:\n","        swaps = [(\"Paris\",\"London\"),(\"COVID-19\",\"Influenza\"),(\"Einstein\",\"Newton\"),\n","                 (\"Mars\",\"Venus\"),(\"UN\",\"NATO\")]\n","        for a,b in swaps:\n","            if a in s: return s.replace(a,b)\n","        return s\n","\n","    def perturb(self, s: str) -> List[str]:\n","        variants = [self._negate(s), self._swap_numbers(s), self._swap_entities(s)]\n","        return list({v for v in variants if v and v != s})\n","\n","    def generate_diverse(self, question: str, n=config.N_CANDIDATES) -> List[str]:\n","        prompt = f\"Question: {question}\\nAnswer:\"\n","        enc = self.tok(prompt, return_tensors=\"pt\", padding=True, return_attention_mask=True)\n","        enc = {k: v.to(self.device) for k,v in enc.items()}\n","        cands = []\n","\n","        with torch.no_grad():\n","            # nucleus + temperature\n","            for t in [0.7, 0.9, 1.1]:\n","                if len(cands)>=n: break\n","                out = self._sample_once(enc, do_sample=True, top_p=0.9, temperature=t)\n","                ans = self._decode_answer(self.tok.decode(out[0], skip_special_tokens=True), prompt)\n","                if ans and ans not in cands: cands.append(ans)\n","\n","            # top-k\n","            for k in [40,60]:\n","                if len(cands)>=n: break\n","                out = self._sample_once(enc, do_sample=True, top_k=k)\n","                ans = self._decode_answer(self.tok.decode(out[0], skip_special_tokens=True), prompt)\n","                if ans and ans not in cands: cands.append(ans)\n","\n","            # beam variants\n","            if len(cands) < n:\n","                ret = min(3, n-len(cands))\n","                outs = self._sample_once(enc, num_beams=3, num_return_sequences=ret, early_stopping=True)\n","                for o in outs:\n","                    ans = self._decode_answer(self.tok.decode(o, skip_special_tokens=True), prompt)\n","                    if ans and ans not in cands: cands.append(ans)\n","\n","        # add perturbations of the best deterministic candidate if available\n","        if cands:\n","            pert = self.perturb(cands[0])\n","            for p in pert:\n","                if len(cands)>=n: break\n","                if p not in cands: cands.append(p)\n","\n","        return cands[:n]\n","\n","# =========================\n","# SGCT Dataset & Collator (with mask)\n","# =========================\n","class SGCTDataset(Dataset):\n","    def __init__(self, items: List[Dict]): self.items = items\n","    def __len__(self): return len(self.items)\n","    def __getitem__(self, i): return self.items[i]\n","\n","class SGCTCollator:\n","    def __init__(self, tok, max_len=config.MAX_LENGTH):\n","        self.tok = tok; self.max_len = max_len\n","    def __call__(self, batch):\n","        factual_inputs, hall_inputs, has_hal = [], [], []\n","        for it in batch:\n","            q = it['question']\n","            factual_inputs.append(f\"Question: {q}\\nAnswer: {it['factual_answer']}\")\n","            if it.get('hallucinated_answer') and it['hallucinated_answer'].strip():\n","                hall_inputs.append(f\"Question: {q}\\nAnswer: {it['hallucinated_answer']}\")\n","                has_hal.append(True)\n","            else:\n","                hall_inputs.append(\"\")\n","                has_hal.append(False)\n","\n","        fac = self.tok(factual_inputs, truncation=True, max_length=self.max_len, padding=True, return_tensors=\"pt\")\n","        hal = self.tok(hall_inputs,   truncation=True, max_length=self.max_len, padding=True, return_tensors=\"pt\")\n","\n","        return {\n","            \"factual_input_ids\": fac[\"input_ids\"], \"factual_attention_mask\": fac[\"attention_mask\"],\n","            \"hallucinated_input_ids\": hal[\"input_ids\"], \"hallucinated_attention_mask\": hal[\"attention_mask\"],\n","            \"has_hallucinated\": torch.tensor(has_hal, dtype=torch.bool)\n","        }\n","\n","# =========================\n","# Enhanced SGCT Trainer\n","# =========================\n","class EnhancedSGCTTrainer:\n","    def __init__(self, model_name=config.SGCT_MODEL_NAME, hpm: HPMFilter=None):\n","        self.device = config.DEVICE\n","        self.hpm = hpm\n","        self.tok = GPT2TokenizerFast.from_pretrained(model_name)\n","        self.model = GPT2LMHeadModel.from_pretrained(model_name)\n","        if self.tok.pad_token is None:\n","            self.tok.pad_token = self.tok.eos_token\n","            self.model.config.pad_token_id = self.tok.eos_token_id\n","        self.model.to(self.device)\n","        self.generator = CandidateGenerator(model_name)\n","        self.train_losses, self.val_losses = [], []\n","        self.best_val = float(\"inf\")\n","        self.no_improve = 0\n","\n","    # ---- Build contrastive set (ground-truth anchored) ----\n","    def build_pairs(self, qs: List[str], gts: List[str]) -> List[Dict]:\n","        pairs = []\n","        for q, gt in tqdm(list(zip(qs, gts)), desc=\"Building contrastive pairs\"):\n","            cands = self.generator.generate_diverse(q)\n","            filt = self.hpm.filter_candidates(q, cands)\n","            factual_anchor = gt\n","            # add a couple of extra factuals and a capped number of hallucinations\n","            for fa in filt[\"factual\"][:2]:\n","                pairs.append({\"question\": q, \"factual_answer\": fa})\n","            for ha in filt[\"hallucinated\"][:3]:\n","                pairs.append({\"question\": q, \"factual_answer\": factual_anchor, \"hallucinated_answer\": ha})\n","            # always include ground truth for LM objective\n","            pairs.append({\"question\": q, \"factual_answer\": factual_anchor})\n","        random.shuffle(pairs)\n","        return pairs\n","\n","    # ---- Cosine-margin push-away loss ----\n","    def contrastive_loss(self, factual_hidden, halluc_hidden, margin=0.3):\n","        if halluc_hidden is None or factual_hidden is None:\n","            return torch.tensor(0.0, device=self.device)\n","        # last-token pooling to avoid pad influence\n","        f = factual_hidden[:, -1, :]\n","        h = halluc_hidden[:, -1, :]\n","        f = F.normalize(f, dim=-1); h = F.normalize(h, dim=-1)\n","        cos = (f * h).sum(dim=-1)\n","        return F.relu(cos - margin).mean()\n","\n","    # ---- One training step ----\n","    def train_step(self, batch, alpha, beta, gamma):\n","        fac_ids = batch[\"factual_input_ids\"].to(self.device)\n","        fac_mask= batch[\"factual_attention_mask\"].to(self.device)\n","        hal_ids = batch[\"hallucinated_input_ids\"].to(self.device)\n","        hal_mask= batch[\"hallucinated_attention_mask\"].to(self.device)\n","        has_hal = batch[\"has_hallucinated\"].to(self.device)\n","\n","        # factual likelihood + hidden states\n","        out_f = self.model(input_ids=fac_ids, attention_mask=fac_mask,\n","                           labels=fac_ids, output_hidden_states=True)\n","        loss_lh = out_f.loss\n","        factual_hidden = out_f.hidden_states[-1]\n","\n","        loss_ul = torch.tensor(0.0, device=self.device)\n","        loss_ct = torch.tensor(0.0, device=self.device)\n","\n","        # UL/Contrastive only for rows that truly have hallucinated text\n","        if has_hal.any():\n","            hal_ids_sel  = hal_ids[has_hal]\n","            hal_mask_sel = hal_mask[has_hal]\n","            fac_hid_sel  = factual_hidden[has_hal]\n","\n","            out_h = self.model(input_ids=hal_ids_sel, attention_mask=hal_mask_sel,\n","                               output_hidden_states=True)\n","            logits_h = out_h.logits\n","\n","            # token-wise unlikelihood on hallucinated sequence (only high prob tokens)\n","            probs_h = F.softmax(logits_h[:, :-1], dim=-1)                    # [B, T-1, V]\n","            labels_shift = hal_ids_sel[:, 1:]\n","            token_probs = torch.gather(probs_h, 2, labels_shift.unsqueeze(-1)).squeeze(-1)\n","            high = token_probs > 0.05\n","            if high.any():\n","                loss_ul = -torch.log(1 - token_probs[high] + 1e-8).mean()\n","\n","            # contrastive (push factual away from hallucinated)\n","            loss_ct = self.contrastive_loss(fac_hid_sel, out_h.hidden_states[-1])\n","\n","        total = alpha*loss_lh + beta*loss_ul + gamma*loss_ct\n","        return total, {\"lh\": loss_lh.item(), \"ul\": loss_ul.item(), \"ct\": loss_ct.item()}\n","\n","    # ---- Evaluate via HPM on a validation set (proxy truth = HPM on GT answers) ----\n","    @torch.no_grad()\n","    def evaluate_generation(self, q_list: List[str], a_true: List[str]) -> Dict[str,float]:\n","        self.model.eval()\n","        preds = []\n","        for q in tqdm(q_list, desc=\"Eval generation\"):\n","            prompt = f\"Question: {q}\\nAnswer:\"\n","            enc = self.tok(prompt, return_tensors=\"pt\").to(self.device)\n","            out = self.model.generate(\n","                **enc, max_new_tokens=config.GEN_MAX_NEW_TOKENS,\n","                do_sample=True, top_p=config.GEN_TOP_P, top_k=config.GEN_TOP_K,\n","                temperature=config.GEN_TEMPERATURE,\n","                no_repeat_ngram_size=3, repetition_penalty=1.1,\n","                pad_token_id=self.tok.eos_token_id, eos_token_id=self.tok.eos_token_id\n","            )\n","            text = self.tok.decode(out[0], skip_special_tokens=True)\n","            ans = text.split(\"Answer:\")[-1].strip()\n","            preds.append(ans if ans else \"\")\n","\n","        gen_scores = self.hpm.predict_batch(q_list, preds)\n","        y_pred = [1 if s>=config.FACTUAL_THRESHOLD else 0 for s in gen_scores]\n","\n","        gt_scores = self.hpm.predict_batch(q_list, a_true)\n","        y_true = [1 if s>=config.FACTUAL_THRESHOLD else 0 for s in gt_scores]\n","\n","        acc = accuracy_score(y_true, y_pred)\n","        prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n","        return {\"accuracy\":acc, \"precision\":prec, \"recall\":rec, \"f1\":f1}\n","\n","    # ---- Train with curriculum + early stopping on val F1 ----\n","    def fit(self, train_items, val_items, epochs=config.SGCT_EPOCHS, batch_size=config.BATCH_SIZE, patience=config.PATIENCE):\n","        dl = DataLoader(SGCTDataset(train_items), batch_size=batch_size, shuffle=True,\n","                        collate_fn=SGCTCollator(self.tok))\n","        optim = torch.optim.AdamW(self.model.parameters(), lr=config.LEARNING_RATE, weight_decay=0.01)\n","        total_steps = len(dl)*epochs\n","        sched = get_linear_schedule_with_warmup(optim, int(0.1*total_steps), total_steps)\n","\n","        alpha, beta, gamma = config.ALPHA_LIKELIHOOD, config.BETA_UNLIKELIHOOD, config.GAMMA_CONTRASTIVE\n","\n","        best_f1, best_epoch = -1, -1\n","        metrics_history = {\"train_loss\":[], \"val_loss\":[], \"val_prec\":[], \"val_rec\":[], \"val_f1\":[]}\n","\n","        for ep in range(1, epochs+1):\n","            self.model.train()\n","            total = 0.0; comp = defaultdict(float)\n","            for batch in tqdm(dl, desc=f\"Epoch {ep}/{epochs}\"):\n","                optim.zero_grad()\n","                loss, parts = self.train_step(batch, alpha, beta, gamma)\n","                loss.backward()\n","                torch.nn.utils.clip_grad_norm_(self.model.parameters(), 1.0)\n","                optim.step(); sched.step()\n","                total += loss.item()\n","                for k,v in parts.items(): comp[k]+=v\n","            avg = total/len(dl)\n","            self.train_losses.append(avg)\n","            logger.info(f\"[Epoch {ep}] train_loss={avg:.4f} | lh={comp['lh']/len(dl):.4f} ul={comp['ul']/len(dl):.4f} ct={comp['ct']/len(dl):.4f}\")\n","\n","            # ---- validation via HPM on val questions ----\n","            val_qs  = [it[\"question\"] for it in val_items]\n","            val_ans = [it[\"answer\"] for it in val_items]\n","            val_metrics = self.evaluate_generation(val_qs, val_ans)\n","            val_loss = 1.0 - val_metrics[\"f1\"]\n","            self.val_losses.append(val_loss)\n","\n","            metrics_history[\"train_loss\"].append(avg)\n","            metrics_history[\"val_loss\"].append(val_loss)\n","            metrics_history[\"val_prec\"].append(val_metrics[\"precision\"])\n","            metrics_history[\"val_rec\"].append(val_metrics[\"recall\"])\n","            metrics_history[\"val_f1\"].append(val_metrics[\"f1\"])\n","\n","            logger.info(f\"[Val] acc={val_metrics['accuracy']:.3f} prec={val_metrics['precision']:.3f} rec={val_metrics['recall']:.3f} f1={val_metrics['f1']:.3f}\")\n","\n","            # ---- adaptive curriculum by recall ----\n","            if val_metrics[\"recall\"] < 0.85:\n","                beta = min(beta + 0.05, 0.6)\n","                gamma = min(gamma + 0.05, 0.6)\n","                alpha = max(alpha - 0.05, 0.5)\n","            else:\n","                alpha = min(alpha + 0.02, 1.0)\n","\n","            # ---- early stopping on best F1 ----\n","            if val_metrics[\"f1\"] > best_f1:\n","                best_f1, best_epoch = val_metrics[\"f1\"], ep\n","                self.no_improve = 0\n","                os.makedirs(config.SGCT_FINAL_DIR, exist_ok=True)\n","                self.model.save_pretrained(config.SGCT_FINAL_DIR)\n","                self.tok.save_pretrained(config.SGCT_FINAL_DIR)\n","                logger.info(f\"✅ New best saved (epoch {ep}) | F1={best_f1:.3f}\")\n","            else:\n","                self.no_improve += 1\n","                if self.no_improve >= patience:\n","                    logger.info(f\"Early stopping at epoch {ep} (best epoch {best_epoch}, F1={best_f1:.3f})\")\n","                    break\n","\n","        # plots\n","        epochs_range = range(1, len(metrics_history[\"train_loss\"])+1)\n","        plt.figure(figsize=(10,6))\n","        plt.plot(epochs_range, metrics_history[\"train_loss\"], label=\"Train Loss\")\n","        plt.plot(epochs_range, metrics_history[\"val_loss\"], label=\"Val Loss (1-F1)\")\n","        plt.plot(epochs_range, metrics_history[\"val_prec\"], label=\"Val Precision\")\n","        plt.plot(epochs_range, metrics_history[\"val_rec\"], label=\"Val Recall\")\n","        plt.plot(epochs_range, metrics_history[\"val_f1\"], label=\"Val F1\")\n","        plt.xlabel(\"Epoch\"); plt.ylabel(\"Metric\"); plt.title(\"SGCT Training Metrics\")\n","        plt.grid(); plt.legend(); plt.show()\n","\n","        return metrics_history\n","\n","# =========================\n","# PRE/POST Evaluation Helper (uses GT-answers as HPM proxy truth)\n","# =========================\n","@torch.no_grad()\n","def evaluate_model_with_hpm(generator_tok, generator_model, hpm: HPMFilter, questions: List[str], answers: List[str]) -> Dict[str,float]:\n","    preds = []\n","    device = next(generator_model.parameters()).device\n","    for q in tqdm(questions, desc=\"Eval (pre/post)\"):\n","        prompt = f\"Question: {q}\\nAnswer:\"\n","        enc = generator_tok(prompt, return_tensors=\"pt\").to(device)\n","        out = generator_model.generate(\n","            **enc, max_new_tokens=config.GEN_MAX_NEW_TOKENS,\n","            do_sample=True, top_p=config.GEN_TOP_P, top_k=config.GEN_TOP_K,\n","            temperature=config.GEN_TEMPERATURE,\n","            no_repeat_ngram_size=3, repetition_penalty=1.1,\n","            pad_token_id=generator_tok.eos_token_id, eos_token_id=generator_tok.eos_token_id\n","        )\n","        text = generator_tok.decode(out[0], skip_special_tokens=True)\n","        ans = text.split(\"Answer:\")[-1].strip()\n","        preds.append(ans if ans else \"\")\n","\n","    gen_scores = hpm.predict_batch(questions, preds)\n","    y_pred = [1 if s>=config.FACTUAL_THRESHOLD else 0 for s in gen_scores]\n","\n","    gt_scores = hpm.predict_batch(questions, answers)\n","    y_true = [1 if s>=config.FACTUAL_THRESHOLD else 0 for s in gt_scores]\n","\n","    acc = accuracy_score(y_true, y_pred)\n","    prec, rec, f1, _ = precision_recall_fscore_support(y_true, y_pred, average=\"binary\", zero_division=0)\n","    return {\"accuracy\":acc, \"precision\":prec, \"recall\":rec, \"f1\":f1}\n","\n","# =========================\n","# Threshold sweep utility (optional)\n","# =========================\n","@torch.no_grad()\n","def suggest_best_threshold(hpm: HPMFilter, qs: List[str], gts: List[str]):\n","    scores = hpm.predict_batch(qs, gts)\n","    taus = np.linspace(0.3, 0.9, 25)\n","    best_tau, best_f1, triples = config.FACTUAL_THRESHOLD, -1, []\n","    for t in taus:\n","        y = [1 if s>=t else 0 for s in scores]\n","        # treat HPM-self-consistency via macro: precision=recall since it's self; we score density via balance\n","        p = sum(y)/len(y)\n","        f1_proxy = 2*p*(1-p)  # prefer thresholds that avoid extremes\n","        triples.append((t, p, f1_proxy))\n","        if f1_proxy > best_f1:\n","            best_f1, best_tau = f1_proxy, t\n","    logger.info(f\"Suggested FACTUAL_THRESHOLD={best_tau:.2f} (from density balance)\")\n","    return best_tau\n","\n","# =========================\n","# MAIN execution outside function for global access\n","# =========================\n","\n","# Load HPM\n","hpm = HPMFilter(config.HPM_MODEL_PATH)\n","\n","# Load dataset\n","with open(config.DATASET_PATH, \"r\") as f:\n","    data = json.load(f)\n","df = pd.DataFrame(data)\n","df = df.dropna(subset=[\"question\",\"answer\"]).reset_index(drop=True)\n","\n","# Split into train/val/test on questions\n","train_df, temp_df = train_test_split(df, test_size=0.3, random_state=42)\n","val_df, test_df   = train_test_split(temp_df, test_size=0.5, random_state=42)\n","\n","train_q, train_a = train_df[\"question\"].tolist(), train_df[\"answer\"].tolist()\n","val_q,   val_a   = val_df[\"question\"].tolist(),   val_df[\"answer\"].tolist()\n","test_q,  test_a  = test_df[\"question\"].tolist(),  test_df[\"answer\"].tolist()\n","\n","# (Optional) tune HPM threshold from validation density\n","try:\n","    best_tau = suggest_best_threshold(hpm, val_q, val_a)\n","    config.FACTUAL_THRESHOLD = float(best_tau)\n","    config.HALLUCINATED_THRESHOLD = max(0.1, min(0.5, best_tau - 0.2))\n","    logger.info(f\"Using FACTUAL_THRESHOLD={config.FACTUAL_THRESHOLD:.2f}, HALLUCINATED_THRESHOLD={config.HALLUCINATED_THRESHOLD:.2f}\")\n","except Exception as e:\n","    logger.warning(f\"Threshold sweep skipped: {e}\")\n","\n","# Instantiate trainer\n","sgct = EnhancedSGCTTrainer(config.SGCT_MODEL_NAME, hpm)\n","\n","# ===== Pre-SGCT baseline using HPM judge =====\n","logger.info(\"Baseline (pre-SGCT) generator evaluation\")\n","pre_metrics = evaluate_model_with_hpm(sgct.tok, sgct.model, hpm, test_q, test_a)\n","logger.info(f\"[PRE] acc={pre_metrics['accuracy']:.3f} prec={pre_metrics['precision']:.3f} rec={pre_metrics['recall']:.3f} f1={pre_metrics['f1']:.3f}\")\n","\n","# ===== Build SGCT contrastive pairs (train split only) =====\n","contrastive_pairs = sgct.build_pairs(train_q, train_a)\n","if len(contrastive_pairs) == 0:\n","    logger.error(\"No contrastive pairs produced. Check thresholds or generation.\")\n","    # exit() # Don't exit, just return\n","    # return # This would exit the main function, but not the cell execution\n","    pass # Do nothing and allow the cell to continue, although subsequent steps might fail\n","\n","# ===== Train with curriculum + early stopping on val recall/F1 =====\n","val_items = [{\"question\":q, \"answer\":a} for q,a in zip(val_q, val_a)]\n","history = sgct.fit(contrastive_pairs, val_items, epochs=config.SGCT_EPOCHS, batch_size=config.BATCH_SIZE, patience=config.PATIENCE)\n","\n","# ===== Post-SGCT evaluation =====\n","logger.info(\"Post-SGCT evaluation on test set\")\n","best_tok  = GPT2TokenizerFast.from_pretrained(config.SGCT_FINAL_DIR)\n","best_gen  = GPT2LMHeadModel.from_pretrained(config.SGCT_FINAL_DIR).to(config.DEVICE)\n","post_metrics = evaluate_model_with_hpm(best_tok, best_gen, hpm, test_q, test_a)\n","logger.info(f\"[POST] acc={post_metrics['accuracy']:.3f} prec={post_metrics['precision']:.3f} rec={post_metrics['recall']:.3f} f1={post_metrics['f1']:.3f}\")\n","\n","# Print concise comparison\n","print(\"\\n=== PRE vs POST (HPM-judged factuality on generated answers) ===\")\n","for k in [\"accuracy\",\"precision\",\"recall\",\"f1\"]:\n","    print(f\"{k.capitalize():<10} PRE: {pre_metrics[k]:.3f} | POST: {post_metrics[k]:.3f}\")\n","\n","# Save results\n","os.makedirs(config.SGCT_CHECKPOINT_DIR, exist_ok=True)\n","with open(os.path.join(config.SGCT_CHECKPOINT_DIR, \"pre_post_results.json\"), \"w\") as f:\n","    json.dump({\"pre\":pre_metrics, \"post\":post_metrics}, f, indent=2)\n","\n","# Removed the if __name__ == \"__main__\": block\n","# main() # No need to call main() now that the code is global"]},{"cell_type":"code","source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_recall_curve, f1_score\n","\n","# === Step 0: Rebuild val_items if not already defined ===\n","try:\n","    val_items\n","except NameError:\n","    val_items = [{\"question\":q, \"answer\":a} for q,a in zip(val_q, val_a)]\n","    print(f\"val_items rebuilt with {len(val_items)} examples\")\n","\n","# === Step 1: Collect validation predictions and HPM scores ===\n","val_qs  = [it[\"question\"] for it in val_items]\n","val_ans = [it[\"answer\"] for it in val_items]\n","\n","\n","gen_preds, gen_scores = [], []\n","best_tok  = GPT2TokenizerFast.from_pretrained(config.SGCT_FINAL_DIR)\n","best_gen  = GPT2LMHeadModel.from_pretrained(config.SGCT_FINAL_DIR).to(config.DEVICE)\n","\n","for q in val_qs:\n","    prompt = f\"Question: {q}\\nAnswer:\"\n","    enc = best_tok(prompt, return_tensors=\"pt\").to(config.DEVICE)\n","    out = best_gen.generate(\n","        **enc,\n","        max_new_tokens=config.GEN_MAX_NEW_TOKENS,\n","        do_sample=True,\n","        top_p=config.GEN_TOP_P,\n","        top_k=config.GEN_TOP_K,\n","        temperature=config.GEN_TEMPERATURE,\n","        pad_token_id=best_tok.eos_token_id\n","    )\n","    text = best_tok.decode(out[0], skip_special_tokens=True)\n","    ans = text.split(\"Answer:\")[-1].strip()\n","    gen_preds.append(ans)\n","\n","# HPM scores = probability factual\n","gen_scores = hpm.predict_batch(val_qs, gen_preds)\n","y_true = np.ones(len(gen_scores))  # proxy: val answers are factual\n","\n","# === Step 2: Threshold sweep ===\n","thresholds = np.linspace(0, 1, 101)\n","f1s, precisions, recalls = [], [], []\n","\n","for t in thresholds:\n","    y_pred = [1 if s >= t else 0 for s in gen_scores]\n","    f1s.append(f1_score(y_true, y_pred, zero_division=0))\n","    p, r, _ = precision_recall_curve(y_true, gen_scores)\n","    precisions.append(p.mean())\n","    recalls.append(r.mean())\n","\n","best_idx = int(np.argmax(f1s))\n","best_tau, best_f1 = thresholds[best_idx], f1s[best_idx]\n","\n","print(f\"Best threshold τ = {best_tau:.2f} with F1 = {best_f1:.3f}\")\n","\n","# === Step 3: Plots ===\n","# Precision–Recall curve\n","p, r, _ = precision_recall_curve(y_true, gen_scores)\n","plt.figure(figsize=(6,5))\n","plt.plot(r, p, label=\"PR curve\")\n","plt.xlabel(\"Recall\")\n","plt.ylabel(\"Precision\")\n","plt.title(\"Precision–Recall Curve (Validation)\")\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","# F1 vs threshold\n","plt.figure(figsize=(6,5))\n","plt.plot(thresholds, f1s, label=\"F1 score\")\n","plt.axvline(best_tau, color=\"r\", linestyle=\"--\", label=f\"Best τ={best_tau:.2f}\")\n","plt.xlabel(\"Decision threshold τ\")\n","plt.ylabel(\"F1 score\")\n","plt.title(\"F1 vs Threshold (Validation)\")\n","plt.legend()\n","plt.grid()\n","plt.show()\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":384},"id":"qlOJBCpVi_Jb","executionInfo":{"status":"error","timestamp":1756906722874,"user_tz":0,"elapsed":3550,"user":{"displayName":"Bruce Jnr","userId":"02572787615479852412"}},"outputId":"1884fe3d-538b-426c-ab77-9bf336640ba2"},"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'val_q' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-485556000.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mval_items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'val_items' is not defined","\nDuring handling of the above exception, another exception occurred:\n","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-485556000.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mval_items\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mNameError\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0mval_items\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mq\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0ma\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mval_q\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_a\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"val_items rebuilt with {len(val_items)} examples\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'val_q' is not defined"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":211},"id":"8dc21b18","executionInfo":{"status":"error","timestamp":1756907507384,"user_tz":0,"elapsed":5403,"user":{"displayName":"Bruce Jnr","userId":"02572787615479852412"}},"outputId":"c294ce9a-d9ff-4fcc-9e52-4faa8a8df7f5"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","from sklearn.metrics import precision_recall_curve, f1_score\n","\n","# === Step 0: Rebuild val_items if not already defined ===\n","# val_items is now defined globally from the previous cell\n","# try:\n","#     val_items\n","# except NameError:\n","#     val_items = [{\"question\":q, \"answer\":a} for q,a in zip(val_q, val_a)]\n","#     print(f\"val_items rebuilt with {len(val_items)} examples\")\n","\n","# === Step 1: Collect validation predictions and HPM scores ===\n","val_qs  = [it[\"question\"] for it in val_items]\n","val_ans = [it[\"answer\"] for it in val_items]\n","\n","\n","gen_preds, gen_scores = [], []\n","# best_tok and best_gen are now defined globally from the previous cell\n","# best_tok  = GPT2TokenizerFast.from_pretrained(config.SGCT_FINAL_DIR)\n","# best_gen  = GPT2LMHeadModel.from_pretrained(config.SGCT_FINAL_DIR).to(config.DEVICE)\n","\n","for q in tqdm(val_qs, desc=\"Collecting validation predictions\"): # Added tqdm for progress bar\n","    prompt = f\"Question: {q}\\nAnswer:\"\n","    enc = best_tok(prompt, return_tensors=\"pt\").to(config.DEVICE)\n","    out = best_gen.generate(\n","        **enc,\n","        max_new_tokens=config.GEN_MAX_NEW_TOKENS,\n","        do_sample=True,\n","        top_p=config.GEN_TOP_P,\n","        top_k=config.GEN_TOP_K,\n","        temperature=config.GEN_TEMPERATURE,\n","        pad_token_id=best_tok.eos_token_id\n","    )\n","    text = best_tok.decode(out[0], skip_special_tokens=True)\n","    ans = text.split(\"Answer:\")[-1].strip()\n","    gen_preds.append(ans)\n","\n","# HPM scores = probability factual\n","# hpm is now defined globally from the previous cell\n","gen_scores = hpm.predict_batch(val_qs, gen_preds)\n","y_true = np.ones(len(gen_scores))  # proxy: val answers are factual\n","\n","# === Step 2: Threshold sweep ===\n","thresholds = np.linspace(0, 1, 101)\n","f1s = []\n","# Removed precisions and recalls lists as they were not used correctly for plotting below\n","# precisions, recalls = [], []\n","\n","for t in thresholds:\n","    y_pred = [1 if s >= t else 0 for s in gen_scores]\n","    f1s.append(f1_score(y_true, y_pred, zero_division=0))\n","    # Removed incorrect calculation of precisions and recalls\n","    # p, r, _ = precision_recall_curve(y_true, gen_scores)\n","    # precisions.append(p.mean())\n","    # recalls.append(r.mean())\n","\n","best_idx = int(np.argmax(f1s))\n","best_tau, best_f1 = thresholds[best_idx], f1s[best_idx]\n","\n","print(f\"Best threshold τ = {best_tau:.2f} with F1 = {best_f1:.3f}\")\n","\n","# === Step 3: Plots ===\n","# Precision–Recall curve\n","# Corrected precision_recall_curve usage\n","p, r, _ = precision_recall_curve(y_true, gen_scores)\n","plt.figure(figsize=(6,5))\n","plt.plot(r, p, label=\"PR curve\")\n","plt.xlabel(\"Recall\")\n","plt.ylabel(\"Precision\")\n","plt.title(\"Precision–Recall Curve (Validation)\")\n","plt.legend()\n","plt.grid()\n","plt.show()\n","\n","# F1 vs threshold\n","plt.figure(figsize=(6,5))\n","plt.plot(thresholds, f1s, label=\"F1 score\")\n","plt.axvline(best_tau, color=\"r\", linestyle=\"--\", label=f\"Best τ={best_tau:.2f}\")\n","plt.xlabel(\"Decision threshold τ\")\n","plt.ylabel(\"F1 score\")\n","plt.title(\"F1 vs Threshold (Validation)\")\n","plt.legend()\n","plt.grid()\n","plt.show()"],"execution_count":1,"outputs":[{"output_type":"error","ename":"NameError","evalue":"name 'val_items' is not defined","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)","\u001b[0;32m/tmp/ipython-input-667436872.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;31m# === Step 1: Collect validation predictions and HPM scores ===\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mval_qs\u001b[0m  \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"question\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_items\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mval_ans\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mit\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"answer\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mit\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mval_items\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mNameError\u001b[0m: name 'val_items' is not defined"]}]}]}